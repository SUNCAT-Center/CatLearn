{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First CatLearn tutorial.\n",
    "\n",
    "This tutorial is intended to help you get familiar Gaussian process regression, which is implemented in CatLearn.\n",
    "\n",
    "First we set up a known underlying function in one dimension. Then we use it to generate some training data, adding a bit of random noise. Finally we will use CatLearn to train some models and make predictions on some unseen data and benchmark those predictions against the known underlying function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from catlearn.preprocess.scaling import standardize, target_standardize\n",
    "from catlearn.regression import GaussianProcess\n",
    "from catlearn.regression.cost_function import get_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A known underlying function in one dimension.\n",
    "def afunc(x):\n",
    "    \"\"\"Define some polynomial function.\"\"\"\n",
    "    y = x - 50.\n",
    "    p = (y + 4) * (y + 4) * (y + 1) * (y - 1) * (y - 3.5) * (y - 2) * (y - 1)\n",
    "    p += 40. * y + 80. * np.sin(10. * x)\n",
    "    return 1. / 20. * p + 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A number of training points in x.\n",
    "train_points = 33\n",
    "noise_magnitude = 1.\n",
    "\n",
    "# Randomly generate the training datapoints x.\n",
    "train = 7.6 * np.random.sample((train_points, 1)) - 4.2 + 50\n",
    "# Each element in the list train can be referred to as a fingerprint.\n",
    "# Call the underlying function to produce the target values.\n",
    "target = np.array(afunc(train))\n",
    "\n",
    "# Add random noise from a normal distribution to the target values.\n",
    "target += noise_magnitude * np.random.randn(train_points, 1)\n",
    "\n",
    "# Generate test datapoints x.\n",
    "test_points = 513\n",
    "test = np.vstack(np.linspace(np.min(train) - 0.1, np.max(train) + 0.1,\n",
    "                             test_points))\n",
    "\n",
    "# Store standard deviations of the training data and targets.\n",
    "stdx = np.std(train)\n",
    "stdy = np.std(target)\n",
    "tstd = 2.\n",
    "\n",
    "# Standardize the training and test data on the same scale.\n",
    "std = standardize(train_matrix=train,\n",
    "                  test_matrix=test)\n",
    "# Standardize the training targets.\n",
    "train_targets = target_standardize(target)\n",
    "# Note that predictions will now be made on the standardized scale.\n",
    "\n",
    "# Store the known underlying function for plotting.\n",
    "linex = np.linspace(np.min(test), np.max(test), test_points)\n",
    "liney = afunc(linex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1 - biased model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prediction parameters.\n",
    "sdt1 = 0.001\n",
    "# Too large width results in a biased model.\n",
    "w1 = 3.0\n",
    "kdict = {'k1': {'type': 'gaussian', 'width': w1}}\n",
    "# Set up the prediction routine.\n",
    "gp = GaussianProcess(kernel_dict=kdict, regularization=sdt1**2,\n",
    "                     train_fp=std['train'],\n",
    "                     train_target=train_targets['target'],\n",
    "                     optimize_hyperparameters=False)\n",
    "# Do predictions.\n",
    "under_fit = gp.predict(test_fp=std['test'], uncertainty=True)\n",
    "# Scale predictions back to the original scale.\n",
    "under_prediction = np.vstack(under_fit['prediction']) * \\\n",
    "    train_targets['std'] + train_targets['mean']\n",
    "under_uncertainty = np.vstack(under_fit['uncertainty']) * \\\n",
    "    train_targets['std']\n",
    "# Get average errors.\n",
    "error = get_error(under_prediction.reshape(-1), afunc(test).reshape(-1))\n",
    "print('Gaussian linear regression prediction:', error['absolute_average'])\n",
    "# Get confidence interval on predictions.\n",
    "upper = under_prediction + under_uncertainty * tstd\n",
    "lower = under_prediction - under_uncertainty * tstd\n",
    "\n",
    "# Plot example 1\n",
    "plt.plot(linex, liney, '-', lw=1, color='black')\n",
    "plt.plot(train, target, 'o', alpha=0.2, color='black')\n",
    "plt.plot(test, under_prediction, 'b-', lw=1, alpha=0.4)\n",
    "plt.fill_between(np.hstack(test), np.hstack(upper), np.hstack(lower),\n",
    "                interpolate=True, color='blue',\n",
    "                alpha=0.2)\n",
    "plt.title('Biased kernel regression model.  \\n' +\n",
    "          'w: {0:.3f}, r: {1:.3f}'.format(w1 * stdx,\n",
    "                                          sdt1 * stdy))\n",
    "plt.xlabel('Descriptor')\n",
    "plt.ylabel('Response')\n",
    "plt.axis('tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the long length scale of the fit and the low predicted uncertainty.\n",
    "\n",
    "## Model example 2 - over-fitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prediction parameters\n",
    "sdt2 = 0.001\n",
    "# Too small width lead to over-fitting.\n",
    "w2 = 0.03\n",
    "kdict = {'k1': {'type': 'gaussian', 'width': w2}}\n",
    "# Set up the prediction routine.\n",
    "gp = GaussianProcess(kernel_dict=kdict, regularization=sdt2**2,\n",
    "                     train_fp=std['train'],\n",
    "                     train_target=train_targets['target'],\n",
    "                     optimize_hyperparameters=False)\n",
    "# Do predictions.\n",
    "over_fit = gp.predict(test_fp=std['test'], uncertainty=True)\n",
    "# Scale predictions back to the original scale.\n",
    "over_prediction = np.vstack(over_fit['prediction']) * \\\n",
    "    train_targets['std'] + train_targets['mean']\n",
    "over_uncertainty = np.vstack(over_fit['uncertainty']) * \\\n",
    "    train_targets['std']\n",
    "# Get average errors.\n",
    "error = get_error(over_prediction.reshape(-1), afunc(test).reshape(-1))\n",
    "print('Gaussian kernel regression prediction:', error['absolute_average'])\n",
    "# Get confidence interval on predictions.\n",
    "over_upper = over_prediction + over_uncertainty * tstd\n",
    "over_lower = over_prediction - over_uncertainty * tstd\n",
    "\n",
    "# Plot example 2\n",
    "plt.plot(linex, liney, '-', lw=1, color='black')\n",
    "plt.plot(train, target, 'o', alpha=0.2, color='black')\n",
    "plt.plot(test, over_prediction, 'r-', lw=1, alpha=0.4)\n",
    "plt.fill_between(np.hstack(test), np.hstack(over_upper),\n",
    "                np.hstack(over_lower), interpolate=True, color='red',\n",
    "                alpha=0.2)\n",
    "plt.title('Over-fitting kernel regression. \\n' +\n",
    "          'w: {0:.3f}, r: {1:.3f}'.format(w2 * stdx,\n",
    "                                          sdt2 * stdy))\n",
    "plt.xlabel('Descriptor')\n",
    "plt.ylabel('Response')\n",
    "plt.axis('tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the short lenghtscale of the fit and the high uncertainty. Notice also that the function predicts a constant value (the mean) when it the distance to known data is >> the lengthscale.\n",
    "\n",
    "## Model example 3 - Optimized Gaussian Process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the prediction routine and optimize hyperparameters.\n",
    "w3 = 0.1\n",
    "sdt3 = 0.001\n",
    "kdict = {\n",
    "    'k1': {'type': 'gaussian', 'width': [w3]},\n",
    "}\n",
    "gp = GaussianProcess(kernel_dict=kdict, regularization=sdt3**2,\n",
    "                     train_fp=std['train'],\n",
    "                     train_target=train_targets['target'],\n",
    "                     optimize_hyperparameters=True)\n",
    "print('Optimized kernel:', gp.kernel_dict)\n",
    "print(-gp.theta_opt['fun'])\n",
    "# Do the optimized predictions.\n",
    "optimized = gp.predict(test_fp=std['test'], uncertainty=True)\n",
    "# Scale predictions back to the original scale.\n",
    "opt_prediction = np.vstack(optimized['prediction']) * \\\n",
    "    train_targets['std'] + train_targets['mean']\n",
    "opt_uncertainty = np.vstack(optimized['uncertainty']) * \\\n",
    "    train_targets['std']\n",
    "# Get average errors.\n",
    "error = get_error(opt_prediction.reshape(-1), afunc(test).reshape(-1))\n",
    "print('Gaussian kernel regression prediction:', error['absolute_average'])\n",
    "# Get confidence interval on predictions.\n",
    "opt_upper = opt_prediction + opt_uncertainty * tstd\n",
    "opt_lower = opt_prediction - opt_uncertainty * tstd\n",
    "\n",
    "# Plot eample 3\n",
    "plt.plot(linex, liney, '-', lw=1, color='black')\n",
    "plt.plot(train, target, 'o', alpha=0.2, color='black')\n",
    "plt.plot(test, opt_prediction, 'g-', lw=1, alpha=0.4)\n",
    "plt.fill_between(np.hstack(test), np.hstack(opt_upper),\n",
    "                np.hstack(opt_lower), interpolate=True,\n",
    "                color='green', alpha=0.2)\n",
    "plt.title('Optimized GP. \\n w: {0:.3f}, r: {1:.3f}'.format(\n",
    "    gp.kernel_dict['k1']['width'][0] * stdx,\n",
    "    np.sqrt(gp.regularization) * stdy))\n",
    "plt.xlabel('Descriptor')\n",
    "plt.ylabel('Response')\n",
    "plt.axis('tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the lenghtscale is optimized to for the function to fit the data. The uncertainty is high in areas where the data is too sparse.\n",
    "\n",
    "### Experiment and get intuition.\n",
    "\n",
    "Now, try playing around with the `train_points` and `noise_magnitude` variables and rerun the models, to get a feel for the behavior of the Gaussian process.\n",
    "\n",
    "If you are interested in using other Gaussian process codes, such as GPflow, an example has been provided below, for comparison to CatLearn's GP.\n",
    "\n",
    "## Model example 4 - GPflow.\n",
    "\n",
    "*** Requires Tensorflow and GPflow ***\n",
    "\n",
    "GPflow is a Gaussian process code built on top of tensorflow. Therefore it can use certain libraries optimized for GPU's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpflow\n",
    "\n",
    "k = gpflow.kernels.RBF(1, lengthscales=0.1)\n",
    "m = gpflow.models.GPR(np.vstack(std['train']), train_targets['target'],\n",
    "                      kern=k)\n",
    "m.likelihood.variance = 0.00003\n",
    "gpflow.train.ScipyOptimizer().minimize(m)\n",
    "mean, var = m.predict_y(std['test'])\n",
    "# Scale predictions back to the orginal scale.\n",
    "mean = mean * train_targets['std'] + train_targets['mean']\n",
    "std = (var ** 0.5) * train_targets['std']\n",
    "opt_upper = mean + std * tstd\n",
    "opt_lower = mean - std * tstd\n",
    "plt.plot(linex, liney, '-', lw=1, color='black')\n",
    "plt.plot(train, target, 'o', alpha=0.2, color='black')\n",
    "plt.plot(test, mean, 'g-', lw=1, alpha=0.4)\n",
    "plt.fill_between(np.hstack(test), np.hstack(opt_upper),\n",
    "                np.hstack(opt_lower), interpolate=True,\n",
    "                color='purple', alpha=0.2)\n",
    "plt.title('Optimized GPflow')\n",
    "plt.xlabel('Descriptor')\n",
    "plt.ylabel('Response')\n",
    "plt.axis('tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example we used GPflow to do a similar Gaussian process as CatLearn's GP."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
