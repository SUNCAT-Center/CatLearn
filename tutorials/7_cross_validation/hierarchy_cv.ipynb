{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarch Cross-Validation <a name=\"head\"></a>\n",
    "\n",
    "This tutorial will go through setting up a function to perform a hierarchy of cross-validation. This will ultimately allow us to create a learning curve and investigate the learning rate of the model.\n",
    "\n",
    "## Table of Contents\n",
    "[(Back to top)](#head)\n",
    "\n",
    "-   [Data Setup](#data-setup)\n",
    "-   [Prediction Setup](#prediction-setup)\n",
    "-   [Cross-valisation Setup](#cross-validation-setup)\n",
    "-   [Prediction Analysis](#prediction-analysis)\n",
    "-   [Conclusions](#conclusions)\n",
    "\n",
    "## Data Setup <a name=\"data-setup\"></a>\n",
    "[(Back to top)](#head)\n",
    "\n",
    "First, we need to import some functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ase.ga.data import DataConnection\n",
    "\n",
    "from atoml.fingerprint.setup import FeatureGenerator\n",
    "from atoml.cross_validation import Hierarchy\n",
    "from atoml.regression import RidgeRegression, GaussianProcess\n",
    "from atoml.regression.cost_function import get_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can load some data. There is some pre-generated data in an ase-db so first, the atoms objects are loaded into a list. Then they are fed through a feature generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect ase atoms database.\n",
    "gadb = DataConnection('../../data/gadb.db')\n",
    "\n",
    "# Get all relaxed candidates from the db file.\n",
    "all_cand = gadb.get_all_relaxed_candidates(use_extinct=False)\n",
    "print('Loaded {} atoms objects'.format(len(all_cand)))\n",
    "\n",
    "# Generate the feature matrix.\n",
    "fgen = FeatureGenerator()\n",
    "features = fgen.return_vec(all_cand, [fgen.eigenspectrum_vec])\n",
    "print('Generated {} feature matrix'.format(np.shape(features)))\n",
    "\n",
    "# Get the target values.\n",
    "targets = []\n",
    "for a in all_cand:\n",
    "    targets.append(a.info['key_value_pairs']['raw_score'])\n",
    "print('Generated {} target vector'.format(np.shape(targets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that the `all_cand` variable is simply a list of atoms objects. There are no constraints on how this should be set up, the above example is just a succinct method for generating the list.\n",
    "\n",
    "## Prediction Setup <a name=\"prediction-setup\"></a>\n",
    "[(Back to top)](#head)\n",
    "\n",
    "Once the feature matrix and target vector have been generated we can define a prediction function. This will be called on all subsets of data and is expected to take test and training features and targets. The function should return a dictionary with `{'result': list, 'size': list}`. The `result` will typically be an average error and the `size` will be the number of training data points. The first prediction routine that we define utilizes ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rr_predict(train_features, train_targets, test_features, test_targets):\n",
    "    \"\"\"Function to perform the RR predictions.\"\"\"\n",
    "    data = {}\n",
    "\n",
    "    # Set up the ridge regression function.\n",
    "    rr = RidgeRegression(W2=None, Vh=None, cv='loocv')\n",
    "    b = rr.find_optimal_regularization(X=train_features, Y=train_targets)\n",
    "    coef = rr.RR(X=train_features, Y=train_targets, omega2=b)[0]\n",
    "\n",
    "    # Test the model.\n",
    "    sumd = 0.\n",
    "    err = []\n",
    "    for tf, tt in zip(test_features, test_targets):\n",
    "        p = np.dot(coef, tf)\n",
    "        sumd += (p - tt) ** 2\n",
    "        e = ((p - tt) ** 2) ** 0.5\n",
    "        err.append(e)\n",
    "    error = (sumd / len(test_features)) ** 0.5\n",
    "\n",
    "    data['result'] = error\n",
    "    data['size'] = len(train_targets)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define any prediction routine in this format. The following provides a second example with Gaussian process predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gp_predict(train_features, train_targets, test_features, test_targets):\n",
    "    \"\"\"Function to perform the GP predictions.\"\"\"\n",
    "    data = {}\n",
    "    \n",
    "    kdict = {\n",
    "        'k1': {\n",
    "            'type': 'gaussian', 'width': 1., 'scaling': 1., 'dimension': 'single'},\n",
    "        }\n",
    "    gp = GaussianProcess(train_fp=train_features, train_target=train_targets,\n",
    "                         kernel_dict=kdict, regularization=1e-2,\n",
    "                         optimize_hyperparameters=True, scale_data=True)\n",
    "\n",
    "    pred = gp.predict(test_fp=test_features)\n",
    "\n",
    "    data['result'] = get_error(pred['prediction'],\n",
    "                               test_targets)['rmse_average']\n",
    "    data['size'] = len(train_targets)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation Setup <a name=\"cross-validation-setup\"></a>\n",
    "[(Back to top)](#head)\n",
    "\n",
    "Next, we can run the cross-validation on the generated data. In order to allow for flexible storage of large numbers of data subsets, we convert the feature and target arrays to a simple db format. This is performed with the `todb()` function. After this, we split up the db index to define the subsets of data with the `split_index()` function. In this case, the maximum amount of data considered in 1000 data points and the smallest set of data will contain a minimum of 50 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize hierarchy cv class.\n",
    "hv = Hierarchy(db_name='test.sqlite', file_name='hierarchy')\n",
    "# Convert features and targets to simple db format.\n",
    "hv.todb(features=features, targets=targets)\n",
    "# Split the data into subsets.\n",
    "ind = hv.split_index(min_split=50, max_split=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Analysis <a name=\"prediction-analysis\"></a>\n",
    "[(Back to top)](#head)\n",
    "\n",
    "The analysis is first performed with ridge regression. Predictions are made for all subsets of data and the averaged errors plotted against the data size. What is typically observed is that as the size of the data subset increases the error decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the predictions for each subset.\n",
    "pred = hv.split_predict(index_split=ind, predict=rr_predict)\n",
    "\n",
    "# Get mean error at each data size.\n",
    "means, meane = hv.transform_output(pred)\n",
    "\n",
    "# Plot the results.\n",
    "plt.plot(pred[1], pred[0], 'o', c='b', alpha=0.5)\n",
    "plt.plot(means, meane, '-', alpha=0.9, c='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform the same analysis with the Gaussian process predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the predictions for each subset.\n",
    "pred = hv.split_predict(index_split=ind, predict=gp_predict)\n",
    "\n",
    "# Get mean error at each data size.\n",
    "means, meane = hv.transform_output(pred)\n",
    "\n",
    "# Plot the results.\n",
    "plt.plot(pred[1], pred[0], 'o', c='r', alpha=0.5)\n",
    "plt.plot(means, meane, '-', alpha=0.9, c='black')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then clean up the directory and remove saved files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove output.\n",
    "os.remove('hierarchy.pickle')\n",
    "os.remove('test.sqlite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions <a name=\"conclusions\"></a>\n",
    "[(Back to top)](#head)\n",
    "\n",
    "This tutorial has gone through generating a simple set of functions to analyze the data size effect on prediction accuracy for two different models.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
