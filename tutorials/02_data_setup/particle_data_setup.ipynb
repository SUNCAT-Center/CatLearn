{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nanoparticle Feature Setup <a name=\"head\"></a>\n",
    "\n",
    "This tutorial details how to generate features from a data set of nanoparticle atoms objects. The data imported here is for 147-atom AuPt alloyed nanoprticles with varying composition and chemical ordering.\n",
    "\n",
    "## Table of Contents\n",
    "[(Back to top)](#head)\n",
    "\n",
    "-   [Requirements](#requirements)\n",
    "-   [Initialization](#initialization)\n",
    "-   [Generating Feature Vectors](#generating-feature-vectors)\n",
    "-   [Storing Data](#storing-data)\n",
    "-   [Retrieving Date](#retrieving-data)\n",
    "-   [Conclusions](#conclusions)\n",
    "\n",
    "## Requirements <a name=\"requirements\"></a>\n",
    "[(Back to top)](#head)\n",
    "\n",
    "-   [CatLearn](https://github.com/SUNCAT-Center/CatLearn)\n",
    "-   [ASE](https://wiki.fysik.dtu.dk/ase/)\n",
    "-   [numpy](http://www.numpy.org/)\n",
    "\n",
    "## Initialization <a name=\"initialization\"></a>\n",
    "[(Back to top)](#head)\n",
    "\n",
    "To start with we import some functions from ASE and CatLearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from ase.ga.data import DataConnection\n",
    "from ase.io import write\n",
    "\n",
    "from catlearn.api.ase_data_setup import get_unique, get_train\n",
    "from catlearn.api.ase_atoms_api import images_connectivity\n",
    "from catlearn.fingerprint.setup import FeatureGenerator, default_fingerprinters\n",
    "from catlearn.utilities import DescriptorDatabase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Feature Vectors <a name=\"generating-feature-vectors\"></a>\n",
    "[(Back to top)](#head)\n",
    "\n",
    "Some data is imported from an existing `ase.db`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect ase atoms database.\n",
    "gadb = DataConnection('../../data/gadb.db')\n",
    "\n",
    "# Get all relaxed candidates from the db file.\n",
    "all_cand = gadb.get_all_relaxed_candidates(use_extinct=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nanoparticles can be displayed with the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write('particle.png', all_cand[-100])\n",
    "Image('particle.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a couple of functions that will make some selection of data for testing and training. In reality, it is far more robust to do this with some form of cross-validation, but this will suffice for this tutorial. The `get_unique` function will randomly select a defined data size. The `get_train` can then generate a training dataset with replacement. As the naming suggests, it is ensured none of the data in the unique dataset will make it into the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testset = get_unique(atoms=all_cand, size=10, key='raw_score')\n",
    "images_connectivity(testset['atoms']);\n",
    "trainset = get_train(atoms=all_cand, size=30, taken=testset['taken'],\n",
    "                     key='raw_score')\n",
    "images_connectivity(trainset['atoms']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the atoms objects have been prepared, it is possible to generate the feature sets. This can be performed in parallel by setting `nprocs` greater than 1. If `None` is set then all available cores will be utilized. Two functions can be called to generate the feature sets `return_vec` and the feature names `return_names`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = FeatureGenerator(atom_types=[78, 79], nprocs=1,\n",
    "                             element_parameters=['atomic_radius', 'mass'])\n",
    "feature_names = default_fingerprinters(generator, 'fragment')\n",
    "data = generator.return_vec(trainset['atoms'], feature_names)\n",
    "vec_names = generator.return_names(feature_names)\n",
    "\n",
    "# Sanitize names for database.\n",
    "for i, f in enumerate(vec_names):\n",
    "    vec_names[i] = ('f' + f).replace('-', '_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing Data <a name=\"storing-data\"></a>\n",
    "[(Back to top)](#head)\n",
    "\n",
    "Once the features have been generated, it is possible to store them in a database. This initial implementation of the db isn't so optimal but is a simple way of temporarily storing all the feature vectors. The following lines of code will initialize the db."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define variables for database to store system descriptors.\n",
    "descriptors = list(vec_names)\n",
    "targets = ['Energy']\n",
    "column_names = descriptors + targets\n",
    "\n",
    "# Set up the database to save system descriptors.\n",
    "dd = DescriptorDatabase(db_name='vec_store.sqlite')\n",
    "dd.create_db(names=column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below will then reformat the data slightly, before storing it in the db. In this case the targets are stored in the `atoms.info['key_value_pairs']['raw_score']`. To keep track of where the feature vectors come from, the atoms UUID is utilized. This is stored in `atoms.info['unique_id']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put data in correct format to be inserted into database.\n",
    "new_data = []\n",
    "for d, a in zip(data, trainset['atoms']):\n",
    "    new_data.append([a.info['unique_id']] + list(d) +\n",
    "    [a.info['key_value_pairs']['raw_score']])\n",
    "\n",
    "# Fill the database with the data.\n",
    "dd.fill_db(descriptor_names=column_names, data=new_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to update the db as required. In the following, we just append some random variables from to the db."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.create_column(new_column=['random'])\n",
    "\n",
    "# Add new name to list of descriptors.\n",
    "column_names += ['random']\n",
    "\n",
    "for i in dd.query_db(names=['uuid']):\n",
    "    dd.update_descriptor(\n",
    "        descriptor='random', new_data=random.random(), unique_id=i[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Date <a name=\"retrieving-data\"></a>\n",
    "[(Back to top)](#head)\n",
    "\n",
    "To check what data is now stored in the db, the following can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stored = dd.get_column_names()\n",
    "print('\\nretrieved column names:\\n{}'.format(stored))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve the features and targets, the following can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test out the database functions.\n",
    "train_fingerprint = dd.query_db(names=descriptors)\n",
    "train_target = dd.query_db(names=targets)\n",
    "\n",
    "print('\\nfeature data for candidates: \\n{}'.format(train_fingerprint[:10, :]))\n",
    "print('\\ntarget data for candidates: \\n{}'.format(train_target[:10, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the folder can be tidied up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove('vec_store.sqlite')\n",
    "os.remove('particle.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions <a name=\"conclusions\"></a>\n",
    "[(Back to top)](#head)\n",
    "\n",
    "This tutorial has demonstrated how to generate feature vectors for nanoparticle atoms objects and store that data in a temporary db."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
