{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-fold Cross-Validation <a name=\"head\"></a>\n",
    "\n",
    "This tutorial will go through setting up a function to perform k-fold cross-validation. This is a very common way to assess the performance of a model.\n",
    "\n",
    "## Table of Contents\n",
    "[(Back to top)](#head)\n",
    "\n",
    "-   [Data Setup](#data-setup)\n",
    "-   [Subset Generation](#subset-generation)\n",
    "-   [Prediction Setup](#prediction-setup)\n",
    "-   [Cross-valisation Setup](#cross-validation-setup)\n",
    "-   [Prediction Analysis](#prediction-analysis)\n",
    "-   [Conclusions](#conclusions)\n",
    "\n",
    "## Data Setup <a name=\"data-setup\"></a>\n",
    "[(Back to top)](#head)\n",
    "\n",
    "First, we need to import some functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ase.ga.data import DataConnection\n",
    "\n",
    "from catlearn.fingerprint.setup import FeatureGenerator\n",
    "from catlearn.cross_validation import k_fold\n",
    "from catlearn.cross_validation.k_fold_cv import write_split, read_split\n",
    "from catlearn.regression import RidgeRegression, GaussianProcess\n",
    "from catlearn.regression.cost_function import get_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can load some data. There is some pre-generated data in an ase-db so first, the atoms objects are loaded into a list. Then they are fed through a feature generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect ase atoms database.\n",
    "gadb = DataConnection('../../data/gadb.db')\n",
    "\n",
    "# Get all relaxed candidates from the db file.\n",
    "all_cand = gadb.get_all_relaxed_candidates(use_extinct=False)\n",
    "print('Loaded {} atoms objects'.format(len(all_cand)))\n",
    "\n",
    "# Generate the feature matrix.\n",
    "fgen = FeatureGenerator()\n",
    "features = fgen.return_vec(all_cand, [fgen.eigenspectrum_vec])\n",
    "print('Generated {} feature matrix'.format(np.shape(features)))\n",
    "\n",
    "# Get the target values.\n",
    "targets = []\n",
    "for a in all_cand:\n",
    "    targets.append(a.info['key_value_pairs']['raw_score'])\n",
    "print('Generated {} target vector'.format(np.shape(targets)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to note that the `all_cand` variable is simply a list of atoms objects. There are no constraints on how this should be set up, the above example is just a succinct method for generating the list.\n",
    "\n",
    "## Subset Generation <a name=\"subset-generation\"></a>\n",
    "[(Back to top)](#head)\n",
    "\n",
    "Once the data has been generated, it is necessary to split the training features and training targets into k-folds. This can be achieved using a function in CatLearn with the `k_fold` function. Here is is possible to provide feature data, target data and the number of folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fsplit, tsplit = k_fold(features=features, targets=targets, nsplit=5)\n",
    "\n",
    "print('k_fold has generated {} subsets of features.'.format(len(fsplit)))\n",
    "for index in range(len(fsplit)):\n",
    "    print('    subset {0} has shape {1}'.format(index, np.shape(fsplit[index])))\n",
    "    \n",
    "print('\\nk_fold has generated {} subsets of targets.'.format(len(tsplit)))\n",
    "for index in range(len(tsplit)):\n",
    "    print('    subset {0} has shape {1}'.format(index, np.shape(tsplit[index])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are interested in saving this data, it is possible to write a JSON or pickle file. This is achieved using the following functions to write and read the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First write a json file.\n",
    "write_split(features=fsplit, targets=tsplit, fname='kfoldSave', fformat='json')\n",
    "\n",
    "# Read data back in.\n",
    "fread, tread = read_split(fname='kfoldSave', fformat='json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data has been divided into subsets, it is possible to analyze the models being trained.\n",
    "\n",
    "## Prediction Setup <a name=\"prediction-setup\"></a>\n",
    "[(Back to top)](#head)\n",
    "\n",
    "Once the data split has been generated we can define a prediction function. This will be called on all subsets of data and is expected to take test and training features and targets. The function then returns a dictionary with `{'result': list, 'size': list}`. The `result` is an average error and the `size` will be the number of training data points. The following provides an example with Gaussian process predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gp_predict(train_features, train_targets, test_features, test_targets):\n",
    "    \"\"\"Function to perform the GP predictions.\"\"\"\n",
    "    data = {}\n",
    "    \n",
    "    kdict = {\n",
    "        'k1': {\n",
    "            'type': 'gaussian', 'width': 1., 'scaling': 1., 'dimension': 'single'},\n",
    "        }\n",
    "    gp = GaussianProcess(train_fp=train_features, train_target=train_targets,\n",
    "                         kernel_dict=kdict, regularization=1e-2,\n",
    "                         optimize_hyperparameters=True, scale_data=True)\n",
    "\n",
    "    pred = gp.predict(test_fp=test_features)\n",
    "\n",
    "    data['result'] = get_error(pred['prediction'],\n",
    "                               test_targets)['rmse_average']\n",
    "    data['size'] = len(train_targets)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation Setup <a name=\"cross-validation-setup\"></a>\n",
    "[(Back to top)](#head)\n",
    "\n",
    "Next, we can run the cross-validation on the generated data. In order to allow for flexible storage of large numbers of data subsets, we convert the feature and target arrays to a simple db format. This is performed with the `todb()` function. After this, we split up the db index to define the subsets of data with the `split_index()` function. In this case, the maximum amount of data considered in 1000 data points and the smallest set of data will contain a minimum of 50 data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = list(range(len(fsplit))), []\n",
    "for index in x:\n",
    "    fcopy, tcopy = fsplit.copy(), tsplit.copy()\n",
    "    ftest, ttest = fcopy.pop(index), tcopy.pop(index)\n",
    "    \n",
    "    fcopy = np.concatenate(fcopy, axis=0)\n",
    "    tcopy = np.concatenate(tcopy, axis=0)\n",
    "    \n",
    "    y.append(gp_predict(fcopy, tcopy, ftest, ttest)['result'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Analysis <a name=\"prediction-analysis\"></a>\n",
    "[(Back to top)](#head)\n",
    "\n",
    "We can then visualize how the error varies acorss the different subsets of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, '-')\n",
    "plt.xlabel('Data Subset')\n",
    "plt.ylabel('Average Error')\n",
    "\n",
    "print('Averaged error: {}'.format(sum(y)/float(len(y))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then clean up the directory and remove saved files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove output.\n",
    "os.remove('kfoldSave.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions <a name=\"conclusions\"></a>\n",
    "[(Back to top)](#head)\n",
    "\n",
    "This tutorial has gone through generating a simple set of functions to perform k-fold cross-validation.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
