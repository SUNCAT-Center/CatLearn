{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fourth CatLearn tutorial.\n",
    "\n",
    "This tutorial is intended to give further intuition for Gaussian processes.\n",
    "\n",
    "Like in tutorial 3, we set up a known underlying function with two training features and one output feature, we generate training and test data and calculate predictions and errors.\n",
    "\n",
    "We will compare the results of linear ridge regression, Gaussian linear kernel regression and finally a Gaussian process with the usual squared exponential kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "from catlearn.preprocess.scaling import standardize, target_standardize\n",
    "from catlearn.regression import GaussianProcess\n",
    "from catlearn.regression.cost_function import get_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A known underlying function in two dimensions\n",
    "def afunc2d(x):\n",
    "    \"\"\"2D linear function (plane).\"\"\"\n",
    "    return 3. * x[:, 0] - 1. * x[:, 1] + 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up data.\n",
    "# A number of training points in x.\n",
    "train_points = 30\n",
    "# Magnitude of the noise.\n",
    "noise_magnitude = 0.3\n",
    "\n",
    "# Randomly generate the training datapoints x.\n",
    "train_d1 = 2 * (np.random.random_sample(train_points) - 0.5)\n",
    "train_d2 = 2 * (np.random.random_sample(train_points) - 0.5)\n",
    "train_x1, train_x2 = np.meshgrid(train_d1, train_d2)\n",
    "train = np.hstack([np.vstack(train_d1), np.vstack(train_d2)])\n",
    "\n",
    "# Each element in the list train can be referred to as a fingerprint.\n",
    "# Call the underlying function to produce the target values.\n",
    "target = np.array(afunc2d(train))\n",
    "\n",
    "# Add random noise from a normal distribution to the target values.\n",
    "for i in range(train_points):\n",
    "    target[i] += noise_magnitude * np.random.normal()\n",
    "\n",
    "# Generate test datapoints x.\n",
    "test_points = 30\n",
    "test1d = np.vstack(np.linspace(-1.5, 1.5, test_points))\n",
    "test_x1, test_x2 = np.meshgrid(test1d, test1d)\n",
    "test = np.hstack([np.vstack(test_x1.ravel()), np.vstack(test_x2.ravel())])\n",
    "\n",
    "print(np.shape(train))\n",
    "print(np.shape(test))\n",
    "print(np.shape(target))\n",
    "\n",
    "# Standardize the training and test data on the same scale.\n",
    "std = standardize(train_matrix=train,\n",
    "                  test_matrix=test)\n",
    "# Standardize the training targets.\n",
    "train_targets = target_standardize(target)\n",
    "# Note that predictions will now be made on the standardized scale.\n",
    "\n",
    "# plot predicted tstd * uncertainties intervals.\n",
    "tstd = 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tstd` variable specifies how many standard deviations we plot.\n",
    "\n",
    "# Model example 1 - Gausian linear kernel regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prediction parameters\n",
    "kdict = {'k1': {'type': 'linear', 'scaling': 1.},\n",
    "         'c1': {'type': 'constant', 'const': 0.}}\n",
    "# Starting guess for the noise parameter\n",
    "sdt1 = noise_magnitude\n",
    "# Set up the gaussian process.\n",
    "gp1 = GaussianProcess(kernel_dict=kdict, regularization=sdt1,\n",
    "                      train_fp=std['train'],\n",
    "                      train_target=train_targets['target'],\n",
    "                      optimize_hyperparameters=True)\n",
    "# Do predictions.\n",
    "linear = gp1.predict(test_fp=std['test'], uncertainty=True)\n",
    "# Put predictions back on real scale.\n",
    "prediction = np.vstack(linear['prediction']) * train_targets['std'] + \\\n",
    "    train_targets['mean']\n",
    "# Put uncertainties back on real scale.\n",
    "uncertainty = np.vstack(linear['uncertainty']) * train_targets['std']\n",
    "# Get confidence interval on predictions.\n",
    "over_upper = prediction + uncertainty * tstd\n",
    "over_lower = prediction - uncertainty * tstd\n",
    "# Plotting.\n",
    "plt3d = plt.figure().gca(projection='3d')\n",
    "\n",
    "# Plot training data.\n",
    "plt3d.scatter(train[:, 0], train[:, 1], target,  color='b')\n",
    "\n",
    "# Plot exact function.\n",
    "plt3d.plot_surface(test_x1, test_x2,\n",
    "                   afunc2d(test).reshape(np.shape(test_x1)),\n",
    "                   alpha=0.3, color='b')\n",
    "# Plot the uncertainties upper and lower bounds.\n",
    "plt3d.plot_surface(test_x1, test_x2,\n",
    "                   over_upper.reshape(np.shape(test_x1)),\n",
    "                   alpha=0.3, color='r')\n",
    "plt3d.plot_surface(test_x1, test_x2,\n",
    "                   over_lower.reshape(np.shape(test_x1)),\n",
    "                   alpha=0.3, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the upper and lower bounds of the confidence interval predicted by the linear model. It is fairly confident on even beyond the region of the training data set.\n",
    "\n",
    "# Model example 2 - squared exponential kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the prediction routine and optimize hyperparameters.\n",
    "kdict = {'k1': {'type': 'gaussian', 'width': [0.3, 3.]}}\n",
    "# Starting guess for the noise parameter\n",
    "sdt1 = noise_magnitude\n",
    "# Set up the gaussian process.\n",
    "gp2 = GaussianProcess(kernel_dict=kdict, regularization=sdt1,\n",
    "                      train_fp=std['train'],\n",
    "                      train_target=train_targets['target'],\n",
    "                      optimize_hyperparameters=True)\n",
    "# Do the optimized predictions.\n",
    "gaussian = gp2.predict(test_fp=std['test'], uncertainty=True)\n",
    "# Put predictions back on real scale.\n",
    "prediction = np.vstack(gaussian['prediction']) * train_targets['std'] + \\\n",
    "    train_targets['mean']\n",
    "# Put uncertainties back on real scale.\n",
    "uncertainty = np.vstack(gaussian['uncertainty']) * train_targets['std']\n",
    "# Get confidence interval on predictions.\n",
    "gp_upper = prediction + uncertainty * tstd\n",
    "gp_lower = prediction - uncertainty * tstd\n",
    "# Plotting.\n",
    "plt3d = plt.figure().gca(projection='3d')\n",
    "\n",
    "# Plot training data.\n",
    "plt3d.scatter(train[:, 0], train[:, 1], target,  color='b')\n",
    "\n",
    "# Plot exact function.\n",
    "plt3d.plot_surface(test_x1, test_x2,\n",
    "                   afunc2d(test).reshape(np.shape(test_x1)),\n",
    "                   alpha=0.3, color='b')\n",
    "# Plot the prediction.\n",
    "plt3d.plot_surface(test_x1, test_x2,\n",
    "                   gp_upper.reshape(np.shape(test_x1)),\n",
    "                   alpha=0.3, color='g')\n",
    "plt3d.plot_surface(test_x1, test_x2,\n",
    "                   gp_lower.reshape(np.shape(test_x1)),\n",
    "                   alpha=0.3, color='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see the confidence interval grows much faster, revealing that the squared exponential kernel is more uncertain outside the region of the training data.\n",
    "\n",
    "### Experiment and get intuition.\n",
    "\n",
    "Now, try playing around with the `train_points`, `noise_magnitude`  and `tstd` variables and rerun the models, to get a feel for the behavior of the Gaussian process and for viewing the various levels of confidence the two models can achieve. \n",
    "\n",
    "\n",
    "# Types of uncertainty\n",
    "\n",
    "A gaussian process gives you epistemic uncertainty. It is assumed to be a constant random noise associated with the observations. Aleatoric uncertainty is not constant although it may be random. It could have an underlying trend making observations less certain in some regions of space than others. Let us try adding aleatoric uncertainty to a toy model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A known underlying function in one dimension.\n",
    "def afunc(x):\n",
    "    \"\"\"Define some polynomial function.\"\"\"\n",
    "    y = x - 50.\n",
    "    p = (y + 4) * (y + 4) * (y + 1) * (y - 1) * (y - 3.5) * (y - 2) * (y - 1)\n",
    "    p += 40. * y + 80. * np.sin(10. * x)\n",
    "    return 1. / 20. * p + 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A number of training points in x.\n",
    "train_points = 65\n",
    "noise_magnitude = 0.3\n",
    "a_noise_magnitude = 10.\n",
    "a_noise_center = 49.\n",
    "a_noise_width = 1.\n",
    "\n",
    "# Randomly generate the training datapoints x.\n",
    "train = 7.6 * np.random.sample((train_points, 1)) - 4.2 + 50\n",
    "\n",
    "# Each element in the list train can be referred to as a fingerprint.\n",
    "# Call the underlying function to produce the target values.\n",
    "target = np.array(afunc(train))\n",
    "\n",
    "# Add random noise from a normal distribution to the target values.\n",
    "target += noise_magnitude * np.random.randn(train_points, 1)\n",
    "# Aleatoric uncertainty:\n",
    "def au(x):\n",
    "    out = a_noise_magnitude * np.exp(-(x-a_noise_center)**2 / (2 * a_noise_width ** 2))\n",
    "    return out\n",
    "target += np.random.randn(train_points, 1) * au(train)\n",
    "\n",
    "# Generate test datapoints x.\n",
    "test_points = 513\n",
    "test = np.vstack(np.linspace(np.min(train) - 0.1, np.max(train) + 0.1,\n",
    "                             test_points))\n",
    "\n",
    "# Store standard deviations of the training data and targets.\n",
    "stdx = np.std(train)\n",
    "stdy = np.std(target)\n",
    "tstd = 2.\n",
    "\n",
    "# Standardize the training and test data on the same scale.\n",
    "std = standardize(train_matrix=train,\n",
    "                  test_matrix=test)\n",
    "# Standardize the training targets.\n",
    "train_targets = target_standardize(target)\n",
    "# Note that predictions will now be made on the standardized scale.\n",
    "\n",
    "# Store the known underlying function for plotting.\n",
    "linex = np.linspace(np.min(test), np.max(test), test_points)\n",
    "liney = afunc(linex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the prediction routine and optimize hyperparameters.\n",
    "w3 = 0.1\n",
    "sdt3 = 0.01\n",
    "kdict = {\n",
    "    'k1': {'type': 'gaussian', 'width': [w3]},\n",
    "}\n",
    "gp = GaussianProcess(kernel_dict=kdict, regularization=sdt3**2,\n",
    "                     train_fp=std['train'],\n",
    "                     train_target=train_targets['target'],\n",
    "                     optimize_hyperparameters=True)\n",
    "print('Optimized kernel:', gp.kernel_dict)\n",
    "print(-gp.theta_opt['fun'])\n",
    "# Do the optimized predictions.\n",
    "optimized = gp.predict(test_fp=std['test'], uncertainty=True)\n",
    "\n",
    "# Scale predictions back to the original scale.\n",
    "opt_prediction = np.vstack(optimized['prediction']) * \\\n",
    "    train_targets['std'] + train_targets['mean']\n",
    "opt_uncertainty = np.vstack(optimized['uncertainty']) * \\\n",
    "    train_targets['std']\n",
    "\n",
    "# Get average errors.\n",
    "error = get_error(opt_prediction.reshape(-1), afunc(test).reshape(-1))\n",
    "print('Gaussian kernel regression prediction:', error['absolute_average'])\n",
    "\n",
    "# Get confidence interval on predictions.\n",
    "opt_upper = opt_prediction + opt_uncertainty * tstd\n",
    "opt_lower = opt_prediction - opt_uncertainty * tstd\n",
    "\n",
    "# Plot eample 3\n",
    "plt.plot(linex, liney, '-', lw=1, color='black')\n",
    "plt.plot(train, target, 'o', alpha=0.2, color='black')\n",
    "plt.plot(test, opt_prediction, 'g-', lw=1, alpha=0.4)\n",
    "plt.fill_between(np.hstack(test), np.hstack(opt_upper),\n",
    "                np.hstack(opt_lower), interpolate=True,\n",
    "                color='green', alpha=0.2)\n",
    "plt.title('Optimized GP. \\n w: {0:.3f}, r: {1:.3f}'.format(\n",
    "    gp.kernel_dict['k1']['width'][0] * stdx,\n",
    "    np.sqrt(gp.regularization) * stdy))\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.axis('tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe how the area around `a_noise_center` is more noisy.\n",
    "\n",
    "Our GP uncertainty is biased, when the data is not equally noisy everywhere. How do we estimate the aleatoric uncertainty?\n",
    "There are several ways. One that does not require any further changes to the GP code is fitting another GP to the magnitude of error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get training errors.\n",
    "optimized_train = gp.predict(test_fp=std['train'], uncertainty=True)\n",
    "predict_train = np.vstack(optimized_train['prediction']) * \\\n",
    "    train_targets['std'] + train_targets['mean']\n",
    "uncertainty_train = np.vstack(optimized_train['uncertainty']) * \\\n",
    "    train_targets['std']\n",
    "train_error = get_error(predict_train.reshape(-1), afunc(train).reshape(-1))\n",
    "gp_aleatoric = GaussianProcess(kernel_dict=kdict, regularization=sdt3**2,\n",
    "                               train_fp=std['train'],\n",
    "                               train_target=train_error['rmse_all']-uncertainty_train.reshape(-1),\n",
    "                               optimize_hyperparameters=True)\n",
    "aleatoric_uncertainty = gp_aleatoric.predict(test_fp=std['test'],\n",
    "                                             uncertainty=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot aleatoric uncertainty\n",
    "plt.plot(linex, au(linex), '-', lw=1, color='black')\n",
    "plt.plot(train, train_error['rmse_all']-uncertainty_train.reshape(-1), 'o', alpha=0.2, color='black')\n",
    "plt.plot(test, aleatoric_uncertainty['prediction'], 'g-', lw=1, alpha=0.4)\n",
    "plt.axis('tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
